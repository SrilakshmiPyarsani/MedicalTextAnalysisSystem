{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BZQecmbwoVK-"
      },
      "outputs": [],
      "source": [
        "\n",
        "Conversation opened. 2 messages. All messages read.\n",
        "\n",
        "Skip to content\n",
        "Using UMBC Mail with screen readers\n",
        "nishanth\n",
        "\n",
        "Submission of Medical Text Analysis System Project - Group 3\n",
        "Inbox\n",
        "\n",
        "Satheesh Meadi\n",
        "Attachments\n",
        "Sat, Nov 23, 8:19 PM\n",
        "to Tony, Nishanth, me, Revanth\n",
        "\n",
        "Dear Professor,\n",
        "\n",
        "I hope this email finds you well. This is Satheesh, on behalf of Group 3, I am pleased to submit our completed project on the Medical Text Analysis System as part of the course requirements.\n",
        "\n",
        "The submission includes the following files:\n",
        "1.⁠ ⁠PDF version of the project code.\n",
        "2.⁠ ⁠Python notebook.\n",
        "3.⁠ ⁠Technical paper (in both Word and PDF formats).\n",
        "\n",
        "Please let us know if there are any issues accessing the files or if you require any additional information.\n",
        "\n",
        "Thank you for the opportunity to work on this project, and we appreciate your guidance throughout the semester.\n",
        "\n",
        "\n",
        "------------------------\n",
        "Thanks & Regards,\n",
        "Satheesh Meadi\n",
        "Graduate Student | Data Scientist\n",
        "Master's In Professional Studies - Data Science\n",
        "University Of Maryland Baltimore County [UMBC],\n",
        "LinkedIn | GitHub | Portfolio\n",
        "\"Harnessing the power of data to tell compelling stories.\"\n",
        "\n",
        "\n",
        " 4 Attachments\n",
        "  •  Scanned by Gmail\n",
        "\n",
        "Dr. Tony Diana <tonydian@umbc.edu>\n",
        "Attachments\n",
        "Sun, Nov 24, 9:32 AM\n",
        "to Satheesh, Nishanth, me, Revanth\n",
        "\n",
        "Hi everyone,\n",
        "Excellent submission and work! As you can see in my attached comments, I have little to say. This deserves an A grade. I am excited to see your final presentation. Thanks.\n",
        "--\n",
        "Dr. Tony DIANA\n",
        "\n",
        " One attachment\n",
        "  •  Scanned by Gmail\n",
        "nishanh1@umbc.edu. Press tab to insert.\n",
        "# '''\n",
        "# Instructions for Running the Project\n",
        "\n",
        "# Requirements:\n",
        "# 1. Google API: The project uses Google API for the Translation feature. Ensure the API key is correctly configured.\n",
        "# 2. Ollama (Chatbot Feature):\n",
        "#    - Download and set up Ollama 3.2 from [Ollama's website](https://ollama.com).\n",
        "#    - Ensure it is running in the background and note the port number it is listening on.\n",
        "#    - Update the local port configuration in the `chat_medical_chatbot` function.\n",
        "# 3. Packages: Ensure the terminal has all necessary packages installed, including `torch`, `fitz`, and `Streamlit`.\n",
        "\n",
        "# Running the Application:\n",
        "# Execute the command:  bash\n",
        "\n",
        "# streamlit run app.py\n",
        "\n",
        "\n",
        "# This will launch the project webpage.\n",
        "\n",
        "\n",
        "# '''\n",
        "\n",
        "# Importing necessary libraries\n",
        "import streamlit as st\n",
        "import zipfile\n",
        "import os\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from transformers import AutoTokenizer, AutoModel, pipeline, AutoModelForQuestionAnswering\n",
        "import torch\n",
        "import requests\n",
        "import shutil\n",
        "import fitz  # PyMuPDF for PDF handling\n",
        "import google.generativeai as genai\n",
        "import openai\n",
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "\n",
        "# Set your API key here for the language translation this is from the google cloud api\n",
        "API_KEY = \"AIzaSyBLUELtvdlQr3T5g5CU8UhN5JSBnDIXyQA\"\n",
        "\n",
        "# Downloaded required NLTK data\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download(\"punkt\")\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "\n",
        "#------------------------#Languages------------------------------------------------------------------\n",
        "\n",
        "LANGUAGES = {\n",
        "    'English': 'en',\n",
        "    'Telugu': 'te',\n",
        "    'Hindi': 'hi',\n",
        "    'Arabic': 'ar',\n",
        "    'Chinese': 'zh',\n",
        "    'Dutch': 'nl',\n",
        "    'Korean': 'ko',\n",
        "    'Russian': 'ru',\n",
        "    'Spanish': 'es',\n",
        "    'Portuguese': 'pt',\n",
        "    'Japanese': 'ja',\n",
        "    'Italian': 'it',\n",
        "    'German': 'de',\n",
        "    'French': 'fr',\n",
        "    'Greek': 'el',\n",
        "    'Thai': 'th'\n",
        "}\n",
        "#<---------------------------------------------------------Sentiment Analysis----------------------------->\n",
        "\n",
        "from transformers import pipeline\n",
        "import streamlit as st\n",
        "\n",
        "class SentimentAnalyzer:\n",
        "    def __init__(self):\n",
        "        # Initialized the sentiment pipeline with the specified model and GPU support\n",
        "        self.analyzer = pipeline(\n",
        "            \"sentiment-analysis\",\n",
        "            model=\"distilbert/distilbert-base-uncased-finetuned-sst-2-english\",\n",
        "            revision=\"714eb0f\",  # Explicitly specifying the model revision\n",
        "            device=0\n",
        "        )\n",
        "\n",
        "    def analyze_sentiment(self, text):\n",
        "        try:\n",
        "            # Handling empty text\n",
        "            if not text or len(text.strip()) == 0:\n",
        "                return None\n",
        "\n",
        "            # Splitting text into smaller chunks (to handle long texts)\n",
        "            max_length = 1000\n",
        "            chunks = [text[i:i+max_length] for i in range(0, len(text), max_length)]\n",
        "\n",
        "            if not chunks:\n",
        "                return None\n",
        "\n",
        "            sentiments = []\n",
        "            for chunk in chunks:\n",
        "                try:\n",
        "                    result = self.analyzer(chunk)\n",
        "                    if result and len(result) > 0:\n",
        "                        sentiments.append(result[0])\n",
        "                except Exception as e:\n",
        "                    st.warning(f\"Chunk analysis failed: {str(e)}\")\n",
        "                    continue\n",
        "\n",
        "            if not sentiments:\n",
        "                return None\n",
        "\n",
        "            # Counting sentiments\n",
        "            positive_count = sum(1 for s in sentiments if s['label'] == 'POSITIVE')\n",
        "            negative_count = sum(1 for s in sentiments if s['label'] == 'NEGATIVE')\n",
        "            neutral_count = len(sentiments) - positive_count - negative_count\n",
        "\n",
        "            total = len(sentiments)\n",
        "            if total == 0:\n",
        "                return None\n",
        "\n",
        "            # Calculating percentages\n",
        "            sentiment_scores = {\n",
        "                'positive': (positive_count / total) * 100,\n",
        "                'negative': (negative_count / total) * 100,\n",
        "                'neutral': (neutral_count / total) * 100\n",
        "            }\n",
        "\n",
        "            # Determined overall sentiment\n",
        "            max_sentiment = max(sentiment_scores.items(), key=lambda x: x[1])\n",
        "\n",
        "            # Mapping sentiment to color and label\n",
        "            sentiment_mapping = {\n",
        "                'positive': {'color': 'green', 'label': 'Positive'},\n",
        "                'negative': {'color': 'red', 'label': 'Negative'},\n",
        "                'neutral': {'color': 'blue', 'label': 'Neutral'}\n",
        "            }\n",
        "\n",
        "            return {\n",
        "                'overall_sentiment': sentiment_mapping[max_sentiment[0]]['label'],\n",
        "                'color': sentiment_mapping[max_sentiment[0]]['color'],\n",
        "                'confidence': max_sentiment[1] / 100,\n",
        "                'breakdown': sentiment_scores\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            st.error(f\"Sentiment analysis failed: {str(e)}\")\n",
        "            return None\n",
        "\n",
        "\n",
        "#<--------------------------------------------------Named Entity Model----------------------------------->\n",
        "class MedicalNER:\n",
        "    def __init__(self):\n",
        "        self.nlp = pipeline(\"ner\", model=\"d4data/biomedical-ner-all\", aggregation_strategy=\"simple\")\n",
        "\n",
        "    def get_named_entities(self, text):\n",
        "        entities = self.nlp(text)\n",
        "        return [(entity['word'], entity['entity_group']) for entity in entities]\n",
        "\n",
        "#<---------------------------------------------------------Translator----------------------------->\n",
        "class Translator:\n",
        "    def __init__(self):\n",
        "        self.translations_cache = {}\n",
        "\n",
        "    def translate_text(self, text, target_language_code):\n",
        "        \"\"\"Translate text using Google Translation API v2 with caching to minimize API calls.\"\"\"\n",
        "        cache_key = (text, target_language_code)\n",
        "\n",
        "        # Check cache first\n",
        "        if cache_key in self.translations_cache:\n",
        "            return self.translations_cache[cache_key]\n",
        "\n",
        "        # API request if no cached result\n",
        "        url = \"https://translation.googleapis.com/language/translate/v2\"\n",
        "        params = {'q': text, 'target': target_language_code, 'key': API_KEY}\n",
        "\n",
        "        try:\n",
        "            response = requests.get(url, params=params)\n",
        "            response_data = response.json()\n",
        "            translation = response_data['data']['translations'][0]['translatedText']\n",
        "\n",
        "            # Cache the result to minimize API calls\n",
        "            self.translations_cache[cache_key] = translation\n",
        "            return translation\n",
        "\n",
        "        except Exception as e:\n",
        "            st.error(f\"Translation failed: {response_data.get('error', {}).get('message', 'Unknown error')}\")\n",
        "            return text\n",
        "\n",
        "#<---------------------------------------------------------PubMed model----------------------------->\n",
        "# # Moves this function outside the class\n",
        "class PubMedBERTSummarizer:\n",
        "    def __init__(self):\n",
        "        self.model_name = \"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\"\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
        "        self.model = AutoModel.from_pretrained(self.model_name)\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        self.model = self.model.to(self.device)\n",
        "\n",
        "    def preprocess_text(self, text):\n",
        "        return re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    @st.cache_data\n",
        "    def get_sentence_embeddings(_self, text):\n",
        "        inputs = _self.tokenizer(text, return_tensors=\"pt\", max_length=512, truncation=True, padding=True)\n",
        "        inputs = {k: v.to(_self.device) for k, v in inputs.items()}\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = _self.model(**inputs)\n",
        "\n",
        "        return outputs.last_hidden_state.mean(dim=1)\n",
        "\n",
        "    def get_pubmedbert_summary(self, text):\n",
        "        try:\n",
        "            processed_text = self.preprocess_text(text)\n",
        "            doc_embedding = self.get_sentence_embeddings(processed_text)\n",
        "            sentences = sent_tokenize(processed_text)\n",
        "\n",
        "            sentence_scores = []\n",
        "            for i, sentence in enumerate(sentences):\n",
        "                sent_embedding = self.get_sentence_embeddings(sentence)\n",
        "                similarity = torch.nn.functional.cosine_similarity(doc_embedding, sent_embedding).item()\n",
        "                sentence_scores.append((i, sentence, similarity))\n",
        "\n",
        "            sorted_sentences = sorted(sentence_scores, key=lambda x: x[2], reverse=True)\n",
        "            selected_sentences = sorted_sentences[:5]  # Select top 5 sentences\n",
        "            summary_sentences = sorted(selected_sentences, key=lambda x: x[0])\n",
        "\n",
        "            summary = ' '.join(sent for _, sent, _ in summary_sentences)\n",
        "            return summary\n",
        "\n",
        "        except Exception as e:\n",
        "            st.error(f\"Summarization error: {str(e)}\")\n",
        "            return text\n",
        "\n",
        "#<-----------------------------------------------------Extracting the Text Data -------------------------------->\n",
        "\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    \"\"\"Extract text from each page of a PDF file using PyMuPDF.\"\"\"\n",
        "    text = \"\"\n",
        "    try:\n",
        "        with fitz.open(pdf_path) as doc:\n",
        "            for page_num in range(doc.page_count):\n",
        "                page = doc[page_num]\n",
        "                text += page.get_text()\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading PDF file {pdf_path}: {e}\")\n",
        "    return text\n",
        "\n",
        "def extract_files(uploaded_file):\n",
        "    extract_to = \"extracted_text_files\"\n",
        "    os.makedirs(extract_to, exist_ok=True)\n",
        "    text_files = []\n",
        "\n",
        "    # Handling zip files\n",
        "    if uploaded_file.name.endswith(\".zip\"):\n",
        "        print(f\"Extracting zip file: {uploaded_file.name}\")\n",
        "        with zipfile.ZipFile(uploaded_file, 'r') as zip_ref:\n",
        "            zip_ref.extractall(extract_to)\n",
        "        text_files = [os.path.join(root, f) for root, _, files in os.walk(extract_to) for f in files if f.endswith('.txt')]\n",
        "        print(f\"Text files extracted from zip: {text_files}\")\n",
        "\n",
        "    # Handling single text files\n",
        "    elif uploaded_file.name.endswith(\".txt\"):\n",
        "        print(f\"Processing text file: {uploaded_file.name}\")\n",
        "        file_path = os.path.join(extract_to, uploaded_file.name)\n",
        "        with open(file_path, \"wb\") as f:\n",
        "            f.write(uploaded_file.getbuffer())\n",
        "        text_files.append(file_path)\n",
        "\n",
        "    # Handling PDF files\n",
        "    elif uploaded_file.name.endswith(\".pdf\"):\n",
        "        print(f\"Processing PDF file: {uploaded_file.name}\")\n",
        "        pdf_path = os.path.join(extract_to, uploaded_file.name)\n",
        "        with open(pdf_path, \"wb\") as f:\n",
        "            f.write(uploaded_file.getbuffer())\n",
        "        pdf_text = extract_text_from_pdf(pdf_path)\n",
        "\n",
        "        # Saves extracted text to a .txt file\n",
        "        text_file_path = os.path.join(extract_to, uploaded_file.name.replace(\".pdf\", \".txt\"))\n",
        "        with open(text_file_path, \"w\", encoding=\"utf-8\") as txt_file:\n",
        "            txt_file.write(pdf_text)\n",
        "        text_files.append(text_file_path)\n",
        "        print(f\"Extracted text saved to: {text_file_path}\")\n",
        "\n",
        "    # Ensures files are found\n",
        "    if not text_files:\n",
        "        raise FileNotFoundError(\"No text files found in the uploaded file.\")\n",
        "\n",
        "    return text_files\n",
        "\n",
        "\n",
        "#<---------------------------------------------------------Chatbot model  with api key from the hugging face----------------------------->\n",
        "\n",
        "\n",
        "# # Medical Chatbot class using Hugging Face Inference API\n",
        "# class MedicalChatbot:\n",
        "#     def __init__(self):\n",
        "#         # Configure the Hugging Face Inference API\n",
        "#         self.API_URL = \"https://api-inference.huggingface.co/models/meta-llama/Llama-3.2-1B\"\n",
        "#         self.headers = {\"Authorization\": \"Bearer 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\"}\n",
        "#         self.conversation_history = []\n",
        "\n",
        "#     def query(self, payload):\n",
        "#         try:\n",
        "#             # Send a POST request to the Hugging Face API\n",
        "#             response = requests.post(self.API_URL, headers=self.headers, json=payload)\n",
        "#             response.raise_for_status()  # Raise an exception for HTTP errors\n",
        "#             return response.json()\n",
        "#         except requests.exceptions.HTTPError as http_err:\n",
        "#             st.error(f\"HTTP error occurred: {http_err}\")\n",
        "#         except requests.exceptions.ConnectionError:\n",
        "#             st.error(\"Failed to connect to the Hugging Face API. Check your internet connection.\")\n",
        "#         except requests.exceptions.Timeout:\n",
        "#             st.error(\"The request timed out. Try again later.\")\n",
        "#         except requests.exceptions.RequestException as req_err:\n",
        "#             st.error(f\"An error occurred: {req_err}\")\n",
        "#         return {\"error\": \"API call failed\"}\n",
        "\n",
        "#     def get_answer(self, question, context):\n",
        "#         # Prepare the payload with context and question\n",
        "#         payload = {\n",
        "#             \"inputs\": f\"Context: {context}\\n\\nQuestion: {question}\",\n",
        "#             \"parameters\": {\n",
        "#                 \"temperature\": 0.5,  # Adjust temperature for creativity\n",
        "#                 \"max_length\": 1000,  # Adjust maximum response length\n",
        "#             },\n",
        "#         }\n",
        "\n",
        "#         # Query the API and process the response\n",
        "#         api_response = self.query(payload)\n",
        "#         if \"generated_text\" in api_response:\n",
        "#             answer = api_response[\"generated_text\"]\n",
        "#         else:\n",
        "#             answer = \"Sorry, I could not process your question.\"\n",
        "\n",
        "#         # Update conversation history\n",
        "#         self.conversation_history.append({\"role\": \"user\", \"content\": question})\n",
        "#         self.conversation_history.append({\"role\": \"assistant\", \"content\": answer})\n",
        "#         return answer\n",
        "\n",
        "#     def clear_history(self):\n",
        "#         # Clear the conversation history\n",
        "#         self.conversation_history = []\n",
        "\n",
        "\n",
        "#<---------------------------------------------------------Chatbot model  with Local host running the Ollama in terminal------------------>\n",
        "\n",
        "# Configure OpenAI API to use Ollama's local server\n",
        "openai.api_base = 'http://localhost:11434/v1'\n",
        "openai.api_key = 'ollama'  # Placeholder key, not used by Ollama\n",
        "\n",
        "# Medical Chatbot class using Ollama for Q&A\n",
        "class MedicalChatbot:\n",
        "    def __init__(self):\n",
        "        self.conversation_history = []\n",
        "\n",
        "    def get_answer(self, question, context):\n",
        "        messages = [\n",
        "            {\"role\": \"system\", \"content\": \"You are a medical expert chatbot. Answer based on the context provided.\"},\n",
        "            {\"role\": \"user\", \"content\": f\"Context: {context}\\n\\nQuestion: {question}\"}\n",
        "        ] + self.conversation_history[-3:]  # Keep only the last 3 interactions for context\n",
        "\n",
        "        # Calling the Ollama model using OpenAI's compatible API structure with \"llama3.2\"\n",
        "        response = openai.ChatCompletion.create(\n",
        "            model=\"llama3.2\",  # Use \"llama3.2\" as the model name for Ollama\n",
        "            messages=messages,\n",
        "            temperature=0.3,\n",
        "            max_tokens=1000\n",
        "        )\n",
        "\n",
        "        answer = response['choices'][0]['message']['content']\n",
        "        self.conversation_history.append({\"role\": \"user\", \"content\": question})\n",
        "        self.conversation_history.append({\"role\": \"assistant\", \"content\": answer})\n",
        "        return answer\n",
        "\n",
        "    def clear_history(self):\n",
        "        self.conversation_history = []\n",
        "\n",
        "\n",
        "#<---------------------------------------------------Initialization----------------------------->\n",
        "\n",
        "def initialize_session_state():\n",
        "    for key, val in [\n",
        "        ('summarizer', PubMedBERTSummarizer()),\n",
        "        ('chatbot', MedicalChatbot()),\n",
        "        ('translator', Translator()),\n",
        "        ('ner', MedicalNER()),\n",
        "        ('sentiment_analyzer', SentimentAnalyzer()),\n",
        "        ('current_summary', None),\n",
        "        ('translated_summary', None),\n",
        "        ('selected_language', 'English'),\n",
        "        ('chat_history', [])  # Stores chat history in session state\n",
        "    ]:\n",
        "        st.session_state.setdefault(key, val)\n",
        "\n",
        "\n",
        "#<----------------------------------Main Function ----------------------------->\n",
        "\n",
        "\n",
        "def main():\n",
        "    st.title(\"Medical Text Analysis System\")\n",
        "    st.write(\"📂 **Upload your medical text file** or ✍️ **enter raw text** using the options in the left sidebar.\")\n",
        "    st.write(\" 🛠️ Use features like summarization, translation, NER, and interactive Q&A!\")\n",
        "\n",
        "    initialize_session_state()\n",
        "\n",
        "    # Sidebar for language selection\n",
        "    target_language = st.sidebar.selectbox(\n",
        "        \"🌐 Select Target Language\",\n",
        "        LANGUAGES.keys(),\n",
        "        index=list(LANGUAGES.keys()).index(st.session_state.selected_language)\n",
        "    )\n",
        "    st.session_state.selected_language = target_language\n",
        "\n",
        "    # Add custom CSS for button styles\n",
        "    st.sidebar.markdown(\n",
        "        \"\"\"\n",
        "        <style>\n",
        "        .button-container {\n",
        "            display: flex;\n",
        "            flex-direction: column;\n",
        "            gap: 10px;\n",
        "        }\n",
        "        .button {\n",
        "            background-color: white;\n",
        "            color: black;\n",
        "            border: 2px solid #ccc;\n",
        "            border-radius: 5px;\n",
        "            padding: 10px;\n",
        "            text-align: center;\n",
        "            cursor: pointer;\n",
        "            font-size: 16px;\n",
        "            transition: background-color 0.3s, color 0.3s;\n",
        "        }\n",
        "        .button:hover {\n",
        "            background-color: orange;\n",
        "            color: white;\n",
        "        }\n",
        "        </style>\n",
        "        \"\"\",\n",
        "        unsafe_allow_html=True,\n",
        "    )\n",
        "\n",
        "\n",
        "    # Initialize session state for selected feature\n",
        "    if \"selected_option\" not in st.session_state:\n",
        "        st.session_state.selected_option = \"Original Text\"\n",
        "\n",
        "\n",
        "    # Sidebar buttons for different functionalities\n",
        "    st.sidebar.title(\"Analysis Options\")\n",
        "    original_text_button = st.sidebar.button(\"📜 Original Text\")\n",
        "    summary_button = st.sidebar.button(\"📝 Summary & Translation\")\n",
        "    ner_button = st.sidebar.button(\"🔍 Named Entity Recognition\")\n",
        "    qa_button = st.sidebar.button(\"💬 Interactive Q&A\")\n",
        "    sentiment_button = st.sidebar.button(\"📊 Sentiment Analysis\")\n",
        "\n",
        "    # Sets the selected option based on button click\n",
        "    if original_text_button:\n",
        "        st.session_state.selected_option = \"Original Text\"\n",
        "    elif summary_button:\n",
        "        st.session_state.selected_option = \"Summary & Translation\"\n",
        "    elif ner_button:\n",
        "        st.session_state.selected_option = \"NER\"\n",
        "    elif qa_button:\n",
        "        st.session_state.selected_option = \"Q&A\"\n",
        "    elif sentiment_button:\n",
        "        st.session_state.selected_option = \"Sentiment Analysis\"\n",
        "\n",
        "    # Input Options\n",
        "    uploaded_files = st.sidebar.file_uploader(\n",
        "        \"Upload medical text file(s)\",\n",
        "        type=[\"txt\", \"zip\", \"pdf\"],\n",
        "        accept_multiple_files=True\n",
        "    )\n",
        "    raw_text = st.sidebar.text_area(\"Or, paste raw text here:\")\n",
        "\n",
        "    # Process user inputs and render the selected analysis\n",
        "    if uploaded_files or raw_text:\n",
        "        try:\n",
        "            text_data = \"\"\n",
        "\n",
        "            # Process uploaded files\n",
        "            if uploaded_files:\n",
        "                for uploaded_file in uploaded_files:\n",
        "                    text_files = extract_files(uploaded_file)\n",
        "                    for text_file in text_files:\n",
        "                        with open(text_file, 'r', encoding='utf-8') as f:\n",
        "                            text_data += f.read() + \"\\n\"\n",
        "\n",
        "            # Uses raw text if provided\n",
        "            if raw_text:\n",
        "                text_data += raw_text\n",
        "\n",
        "            st.write(f\"### Processing Text\")\n",
        "\n",
        "            # Render content based on selected option\n",
        "            if st.session_state.selected_option == \"Original Text\":\n",
        "                st.write(\"Original Text:\")\n",
        "                st.text(text_data)\n",
        "\n",
        "\n",
        "            elif st.session_state.selected_option == \"Summary & Translation\":\n",
        "                st.write(\"📝 **PubMedBERT Summary:**\")\n",
        "                summary = st.session_state.summarizer.get_pubmedbert_summary(text_data)\n",
        "                st.session_state.current_summary = summary\n",
        "                st.write(summary)\n",
        "\n",
        "                if target_language != \"English\":\n",
        "                    translated_text = st.session_state.translator.translate_text(summary, LANGUAGES[target_language])\n",
        "                    st.session_state.translated_summary = translated_text\n",
        "                    st.write(f\"Translation ({target_language}):\\n{translated_text}\")\n",
        "\n",
        "\n",
        "            elif st.session_state.selected_option == \"NER\":\n",
        "                st.write(\"🔍 **Named Entity Recognition (NER)**\")\n",
        "                entities = st.session_state.ner.get_named_entities(text_data)\n",
        "                if entities:\n",
        "                    for entity, entity_type in entities:\n",
        "                        clean_entity = entity.replace(\"#\", \"\")\n",
        "                        st.write(f\"{clean_entity} - {entity_type}\")\n",
        "                else:\n",
        "                    st.write(\"No Named Entities found in the text...\")\n",
        "            elif st.session_state.selected_option == \"Q&A\":\n",
        "                st.write(\"💬**Chat with the Medical Expert Bot**\")\n",
        "                if st.session_state.chat_history:\n",
        "                    for chat in st.session_state.chat_history:\n",
        "                        st.write(f\"🎃 You: {chat['question']}\")\n",
        "                        st.write(f\"💡 Bot: {chat['answer']}\")\n",
        "\n",
        "                question = st.text_input(\"Enter your question:\")\n",
        "                answer_language = st.radio(\"Select answer language:\", [\"English\", target_language], horizontal=True)\n",
        "\n",
        "                if question:\n",
        "                    answer = st.session_state.chatbot.get_answer(question, st.session_state.current_summary)\n",
        "                    if answer_language != \"English\":\n",
        "                        answer = st.session_state.translator.translate_text(answer, LANGUAGES[answer_language])\n",
        "                    st.write(\"🎃 You:\", question)\n",
        "                    st.write(\"💡 Bot:\", answer)\n",
        "                    st.session_state.chat_history.append({\"question\": question, \"answer\": answer})\n",
        "\n",
        "            elif st.session_state.selected_option == \"Sentiment Analysis\":\n",
        "                st.write(\"📊**Sentiment Analysis**\")\n",
        "                if text_data:\n",
        "                    sentiment_result = st.session_state.sentiment_analyzer.analyze_sentiment(text_data)\n",
        "                    if sentiment_result:\n",
        "                        st.markdown(\n",
        "                            f\"<h3 style='color: {sentiment_result['color']}'>\"\n",
        "                            f\"Overall Sentiment: {sentiment_result['overall_sentiment']}</h3>\",\n",
        "                            unsafe_allow_html=True\n",
        "                        )\n",
        "                        st.write(f\"Confidence: {sentiment_result['confidence']*100:.1f}%\")\n",
        "\n",
        "                        col1, col2, col3 = st.columns(3)\n",
        "                        with col1:\n",
        "                            st.markdown(\n",
        "                                f\"<p style='color: green'>Positive: \"\n",
        "                                f\"{sentiment_result['breakdown']['positive']:.1f}%</p>\",\n",
        "                                unsafe_allow_html=True\n",
        "                            )\n",
        "                        with col2:\n",
        "                            st.markdown(\n",
        "                                f\"<p style='color: red'>Negative: \"\n",
        "                                f\"{sentiment_result['breakdown']['negative']:.1f}%</p>\",\n",
        "                                unsafe_allow_html=True\n",
        "                            )\n",
        "                        with col3:\n",
        "                            st.markdown(\n",
        "                                f\"<p style='color: blue'>Neutral: \"\n",
        "                                f\"{sentiment_result['breakdown']['neutral']:.1f}%</p>\",\n",
        "                                unsafe_allow_html=True\n",
        "                            )\n",
        "\n",
        "                        chart_data = pd.DataFrame({\n",
        "                            'Sentiment': ['Positive', 'Negative', 'Neutral'],\n",
        "                            'Percentage': [\n",
        "                                sentiment_result['breakdown']['positive'],\n",
        "                                sentiment_result['breakdown']['negative'],\n",
        "                                sentiment_result['breakdown']['neutral']\n",
        "                            ]\n",
        "                        })\n",
        "\n",
        "                        fig = px.bar(\n",
        "                            chart_data,\n",
        "                            x='Sentiment',\n",
        "                            y='Percentage',\n",
        "                            color='Sentiment',\n",
        "                            color_discrete_map={\n",
        "                                'Positive': 'green',\n",
        "                                'Negative': 'red',\n",
        "                                'Neutral': 'blue'\n",
        "                            }\n",
        "                        )\n",
        "                        st.plotly_chart(fig)\n",
        "\n",
        "                        if sentiment_result['overall_sentiment'] == 'Positive':\n",
        "                            st.success(\"✓ The text contains predominantly positive indicators\")\n",
        "                        elif sentiment_result['overall_sentiment'] == 'Negative':\n",
        "                            st.error(\"⚠️ The text contains significant negative elements\")\n",
        "                        else:\n",
        "                            st.info(\"ℹ️ The text maintains a neutral tone\")\n",
        "                    else:\n",
        "                        st.warning(\"Could not determine sentiment for this text\")\n",
        "                else:\n",
        "                    st.info(\"Please enter or upload text to analyze sentiment\")\n",
        "\n",
        "        except Exception as e:\n",
        "            st.error(f\"An error occurred: {str(e)}\")\n",
        "        finally:\n",
        "            if os.path.exists(\"extracted_text_files\"):\n",
        "                shutil.rmtree(\"extracted_text_files\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "app.py\n",
        "Displaying app.py."
      ]
    }
  ]
}